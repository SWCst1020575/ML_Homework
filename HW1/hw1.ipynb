{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "# Over 2 times of standard deviation would be remove\n",
        "outlierStd = 2\n",
        "# Take 90% of data in training, 10% in validation\n",
        "trainingDataRatio = 0.9\n",
        "\n",
        "def mape(w ,predict, source):\n",
        "    print((np.abs(w[0]+w[1]*source - predict) / source).sum() / source.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    trainingDataset = training_datalist[1:].astype('int')\n",
        "    r, c = trainingDataset.shape\n",
        "    trainNum = round(r*trainingDataRatio)\n",
        "    np.random.shuffle(trainingDataset)\n",
        "    trainingDataset, validationDataset = trainingDataset[:trainNum,:], trainingDataset[trainNum:,:]\n",
        "    return trainingDataset, validationDataset\n",
        "testingDataset = testing_datalist[1:].astype('int')\n",
        "trainingDataset, validationDataset = SplitData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    xMean = np.mean(trainingDataset[:,0])\n",
        "    yMean = np.mean(trainingDataset[:,1])\n",
        "    xStd = np.std(trainingDataset[:,0])\n",
        "    yStd = np.std(trainingDataset[:,1])\n",
        "    outliers = np.logical_and((abs(trainingDataset[:,0] - xMean) < outlierStd * xStd) , (abs(trainingDataset[:,1] - yMean) < outlierStd * yStd))\n",
        "    return trainingDataset[outliers]\n",
        "trainingDatasetRemoveOutlier = PreprocessData()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion():\n",
        "    W = np.c_[np.ones((trainingDatasetRemoveOutlier.shape[0], 1)), trainingDatasetRemoveOutlier[:,0]]\n",
        "    weights = np.linalg.inv(W.T.dot(W)).dot(W.T).dot(trainingDatasetRemoveOutlier[:,1])\n",
        "    return weights\n",
        "\n",
        "weights = MatrixInversion()\n",
        "# Draw the regression line\n",
        "\n",
        "# x = np.linspace(trainingDatasetRemoveOutlier[:,0].min(), trainingDatasetRemoveOutlier[:,0].max(), 1000)\n",
        "# plt.plot(trainingDatasetRemoveOutlier[:,0],trainingDatasetRemoveOutlier[:,1],'bo',markersize=4)\n",
        "# plt.plot(x, weights[0]+weights[1]*x,color='red', linewidth=2.5)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "    return (weights[0]+testingDataset[:,0]*weights[1]).reshape(testingDataset.shape[0],1)\n",
        "output_datalist = MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 346,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0097636244566304 46.3938403512387\n"
          ]
        }
      ],
      "source": [
        "print(weights[1], weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 347,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 348,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 349,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 350,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "# Over 2 times of standard deviation would be remove\n",
        "outlierStd = 2\n",
        "# Gradient descent parameter\n",
        "rate=0.00025\n",
        "iterations = 1000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 351,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "testingDataset = testing_datalist[1:].astype('int')\n",
        "trainingDataset, validationDataset = SplitData()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 352,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "trainingDatasetRemoveOutlier = PreprocessData()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent():\n",
        "    m = trainingDatasetRemoveOutlier.shape[0]\n",
        "    weights = np.random.randn(2)\n",
        "    x = trainingDatasetRemoveOutlier[:,0]\n",
        "    x = np.c_[np.ones((m, 1)), x]\n",
        "    y = trainingDatasetRemoveOutlier[:,1]\n",
        "    for i in range(0,iterations):\n",
        "        gradients = 1/m * x.T.dot(x.dot(weights) - y)\n",
        "        weights = weights - rate * gradients\n",
        "        coefficient_output.append(weights[1])\n",
        "        coefficient_output.append(weights[0])\n",
        "    return weights\n",
        "weights = GradientDescent()\n",
        "coefficient_output = np.array(coefficient_output).reshape(iterations,2)\n",
        "# x = np.linspace(trainingDatasetRemoveOutlier[:,0].min(), trainingDatasetRemoveOutlier[:,0].max(), 1000)\n",
        "# plt.plot(trainingDatasetRemoveOutlier[:,0],trainingDatasetRemoveOutlier[:,1],'bo',markersize=4)\n",
        "# plt.plot(x, weights[0]+weights[1]*x,color='red', linewidth=2.5)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "    return (weights[0]+testingDataset[:,0]*weights[1]).reshape(testingDataset.shape[0],1)\n",
        "output_datalist = MakePrediction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0075070799786043 46.533816553280396\n"
          ]
        }
      ],
      "source": [
        "print(weights[1], weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(training_dataroot, newline='') as csvfile:\n",
        "    training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Miss over 1 data would be removed\n",
        "acceptMissingDataNum = 6\n",
        "\n",
        "outlierStd = 3\n",
        "\n",
        "rate=0.00002\n",
        "iterations = 1200000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acceptData = np.empty((0, 0), bool)\n",
        "# Remove miss over 2 stat data\n",
        "for i in training_datalist:\n",
        "    acceptData = np.append(acceptData, np.array(\n",
        "        [(i != '').sum() >= acceptMissingDataNum]))\n",
        "\n",
        "# Transfer str to number\n",
        "acceptData = training_datalist[acceptData]\n",
        "acceptData[acceptData == ''] = '0'\n",
        "acceptData = acceptData[1:]\n",
        "datanum = acceptData.shape[0]\n",
        "filledDataID = acceptData[:, 0].astype('int').reshape(-1, 1)\n",
        "filledDataTime = acceptData[:, 1].reshape(-1, 1)\n",
        "filledDataTemp = acceptData[:, 2].astype('float').reshape(-1, 1)\n",
        "filledDataHR = acceptData[:, 3].astype('int').reshape(-1, 1)\n",
        "filledDataRP = acceptData[:, 4].astype('int').reshape(-1, 1)\n",
        "filledDataO2 = acceptData[:, 5].astype('int').reshape(-1, 1)\n",
        "filledDataSBP = acceptData[:, 6].astype('int').reshape(-1, 1)\n",
        "\n",
        "# Filled missing data with mean\n",
        "PatientId, _ = np.unique(filledDataID, return_counts=True)\n",
        "for i in PatientId:\n",
        "    filledDataTemp[np.logical_and(filledDataTemp == 0, filledDataID == i)] = round(\n",
        "        filledDataTemp[np.logical_and(filledDataTemp != 0, filledDataID == i)].mean(), 1)\n",
        "    filledDataHR[np.logical_and(filledDataHR == 0, filledDataID == i)] = round(\n",
        "        filledDataHR[np.logical_and(filledDataHR != 0, filledDataID == i)].mean())\n",
        "    filledDataRP[np.logical_and(filledDataRP == 0, filledDataID == i)] = round(\n",
        "        filledDataRP[np.logical_and(filledDataRP != 0, filledDataID == i)].mean())\n",
        "    filledDataO2[np.logical_and(filledDataO2 == 0, filledDataID == i)] = round(\n",
        "        filledDataO2[np.logical_and(filledDataO2 != 0, filledDataID == i)].mean())\n",
        "filledDataTimeToInt = np.empty(0)\n",
        "\n",
        "# remove outlier\n",
        "outliers = np.logical_and((abs(filledDataTemp - filledDataTemp.mean()) < outlierStd * filledDataTemp.std()),\n",
        "                          (abs(filledDataHR - filledDataHR.mean()) < outlierStd * filledDataHR.std()))\n",
        "outliers = np.logical_and(outliers, (abs(\n",
        "    filledDataRP - filledDataRP.mean()) < outlierStd * filledDataRP.std()))\n",
        "outliers = np.logical_and(outliers, (abs(\n",
        "    filledDataO2 - filledDataO2.mean()) < outlierStd * filledDataO2.std()))\n",
        "outliers = np.logical_and(outliers, (abs(\n",
        "    filledDataSBP - filledDataSBP.mean()) < outlierStd * filledDataSBP.std()))\n",
        "\n",
        "# Standardize training data\n",
        "filledDataID = filledDataID[outliers].reshape(-1, 1)\n",
        "filledDataTime = filledDataTime[outliers].reshape(-1, 1)\n",
        "filledDataTemp = ((filledDataTemp[outliers]-filledDataTemp[outliers].mean())/filledDataTemp[outliers].std()).reshape(-1, 1)\n",
        "filledDataHR = ((filledDataHR[outliers]-filledDataHR[outliers].mean())/filledDataHR[outliers].std()).reshape(-1, 1)\n",
        "filledDataRP = ((filledDataRP[outliers]-filledDataRP[outliers].mean())/filledDataRP[outliers].std()).reshape(-1, 1)\n",
        "filledDataO2 = ((filledDataO2[outliers]-filledDataO2[outliers].mean())/filledDataO2[outliers].std()).reshape(-1, 1)\n",
        "filledDataSBP = filledDataSBP[outliers].reshape(-1, 1)\n",
        "# print(filledDataTemp.shape, filledDataSBP.shape)\n",
        "\n",
        "# Transfer date to number\n",
        "for i in filledDataTime:\n",
        "    dateStr = i[0].split(' days ')\n",
        "    timeStr = dateStr[1].split(':')\n",
        "    time = (int(dateStr[0])*24*60 + int(timeStr[0])*60 + int(timeStr[1]))\n",
        "    filledDataTimeToInt = np.append(filledDataTimeToInt, time)\n",
        "filledDataTimeToInt = ((filledDataTimeToInt - filledDataTimeToInt.mean())/filledDataTimeToInt.std()).reshape(-1, 1)\n",
        "\n",
        "# Combine processed data to a matrix\n",
        "trainingData = np.append(filledDataTemp, filledDataHR, axis=1)\n",
        "trainingData = np.append(trainingData, filledDataRP, axis=1)\n",
        "trainingData = np.append(trainingData, filledDataO2, axis=1)\n",
        "trainingData = np.append(trainingData, filledDataTimeToInt, axis=1)\n",
        "\n",
        "\n",
        "def gradientsAdvanced(x,y):\n",
        "    m = x.shape[0]\n",
        "    x = np.c_[np.ones((m, 1)), x]\n",
        "    weights = np.random.randn(x.shape[1], 1)\n",
        "    for i in range(0, iterations):\n",
        "        gradients = 1/m * x.T.dot(x.dot(weights) - y)\n",
        "        weights = weights - rate * gradients\n",
        "    return weights\n",
        "\n",
        "# According to each patients, train their own model\n",
        "weights = {}\n",
        "for i in PatientId:\n",
        "    weights[i]=gradientsAdvanced(trainingData[(filledDataID == i).reshape(-1)],filledDataSBP[(filledDataID == i).reshape(-1)])\n",
        "# print(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.11679466397357496\n"
          ]
        }
      ],
      "source": [
        "# Random take some data to test\n",
        "i = np.sort(np.random.randint(1,m-1,(20,1)),axis=0)\n",
        "source = filledDataSBP[i].reshape(-1,1)\n",
        "\n",
        "predict = np.empty(0,float)\n",
        "count = 0\n",
        "for j in filledDataID[i]:\n",
        "    currentW = (weights[j[0][0]][0]+filledDataTemp[i[count]]*weights[j[0][0]][1]+filledDataHR[i[count]]*weights[j[0][0]][2]+filledDataRP[i[count]]*weights[j[0][0]][3]+filledDataO2[i[count]]*weights[j[0][0]][4]+filledDataTimeToInt[i[count]]*weights[j[0][0]][5])\n",
        "    predict = np.append(predict,currentW[0])\n",
        "    count+=1\n",
        "    \n",
        "predict = predict.reshape(-1,1)\n",
        "# print((np.abs(source - predict) / source).sum() / source.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 362,
      "metadata": {},
      "outputs": [],
      "source": [
        "testingData = testing_datalist[1:]\n",
        "\n",
        "datanum = testingData.shape[0]\n",
        "testingDataID = testingData[:, 0].astype('int').reshape(-1, 1)\n",
        "testingDataTime = testingData[:, 1].reshape(-1, 1)\n",
        "testingDataTemp = testingData[:, 2].astype('float').reshape(-1, 1)\n",
        "testingDataHR = testingData[:, 3].astype('int').reshape(-1, 1)\n",
        "testingDataRP = testingData[:, 4].astype('int').reshape(-1, 1)\n",
        "testingDataO2 = testingData[:, 5].astype('int').reshape(-1, 1)\n",
        "\n",
        "testingDataTemp = ((testingDataTemp-testingDataTemp.mean())/testingDataTemp.std()).reshape(-1, 1)\n",
        "testingDataHR = ((testingDataHR-testingDataHR.mean())/testingDataHR.std()).reshape(-1, 1)\n",
        "testingDataRP = ((testingDataRP-testingDataRP.mean())/testingDataRP.std()).reshape(-1, 1)\n",
        "testingDataO2 = ((testingDataO2-testingDataO2.mean())/testingDataO2.std()).reshape(-1, 1)\n",
        "# Transfer date to number\n",
        "testingDataTimeToNum = np.empty(0)\n",
        "for i in testingDataTime:\n",
        "    dateStr = i[0].split(' days ')\n",
        "    timeStr = dateStr[1].split(':')\n",
        "    time = (int(dateStr[0])*24*60 + int(timeStr[0])*60 + int(timeStr[1]))\n",
        "    testingDataTimeToNum = np.append(testingDataTimeToNum, time)\n",
        "testingDataTimeToNum = ((testingDataTimeToNum - testingDataTimeToNum.mean())/testingDataTimeToNum.std()).reshape(-1, 1)\n",
        "\n",
        "predict = np.empty(0, float)\n",
        "count = 0\n",
        "for j in testingDataID:\n",
        "    currentW = (weights[j[0]][0]+testingDataTemp[count]*weights[j[0]][1]+testingDataHR[count]*weights[j[0]][2] +\n",
        "                testingDataRP[count]*weights[j[0]][3]+testingDataO2[count]*weights[j[0]][4]+testingDataTimeToNum[count]*weights[j[0]][5])\n",
        "    \n",
        "    predict = np.append(predict, currentW[0])\n",
        "    count += 1\n",
        "output_datalist = predict.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 363,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
